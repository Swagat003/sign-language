{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b74a7e-16af-44db-b780-d619b09732e5",
   "metadata": {},
   "source": [
    "***Installations***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d59562-8cf8-4be6-a82f-54e65c56ac60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\swaga\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from opencv-python) (1.24.4)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\swaga\\anaconda3\\lib\\site-packages (0.10.14)\n",
      "Requirement already satisfied: absl-py in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from mediapipe) (21.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from mediapipe) (3.5.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from mediapipe) (1.24.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from mediapipe) (4.25.4)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from mediapipe) (0.5.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from jax->mediapipe) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from jax->mediapipe) (1.9.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from jax->mediapipe) (4.11.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.6->jax->mediapipe) (3.8.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\swaga\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\swaga\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install mediapipe\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd5e1ed-6bd5-4d35-9d1a-a4a649a9d817",
   "metadata": {},
   "source": [
    "***Capture The Images***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d26389-c450-4aa0-b4c0-f079cb336061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data for class 0\n",
      "Collecting data for class 1\n",
      "Collecting data for class 2\n",
      "Collecting data for class 3\n",
      "Collecting data for class 4\n",
      "Collecting data for class 5\n",
      "Stopping data collection.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 6\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 7\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 8\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 9\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 10\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 11\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 12\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 13\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 14\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 15\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 16\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 17\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 18\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 19\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 20\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 21\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 22\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 23\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 24\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n",
      "Collecting data for class 25\n",
      "Error: Could not read frame from camera.\n",
      "Error: Could not read frame from camera.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "DATA_DIR = './data'\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "number_of_classes = 4\n",
    "dataset_size = 100\n",
    "\n",
    "cap = cv2.VideoCapture(0)  \n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video device.\")\n",
    "else:\n",
    "    for j in range(number_of_classes):\n",
    "        class_dir = os.path.join(DATA_DIR, str(j))\n",
    "        if not os.path.exists(class_dir):\n",
    "            os.makedirs(class_dir)\n",
    "\n",
    "        print(f'Collecting data for class {j}')\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Could not read frame from camera.\")\n",
    "                break\n",
    "\n",
    "            cv2.putText(frame, 'Ready? Press \"Q\" to start or \"ESC\" to stop.', (100, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            cv2.imshow('frame', frame)\n",
    "\n",
    "            key = cv2.waitKey(25)\n",
    "            if key == ord('q'):  \n",
    "                break\n",
    "            elif key == 27:  \n",
    "                print(\"Stopping data collection.\")\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                exit()\n",
    "\n",
    "        counter = 0\n",
    "        while counter < dataset_size:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Could not read frame from camera.\")\n",
    "                break\n",
    "\n",
    "            cv2.imshow('frame', frame)\n",
    "            \n",
    "            key = cv2.waitKey(25)\n",
    "            if key == 27: \n",
    "                print(\"Stopping data collection.\")\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                exit()\n",
    "\n",
    "            cv2.imwrite(os.path.join(class_dir, f'{counter}.jpg'), frame)\n",
    "            counter += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a6520-e102-4174-8437-55b4ab5a6099",
   "metadata": {},
   "source": [
    "***Make the Pickle File***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef3b936-3c10-46d8-98b9-ffc1b412a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "DATA_DIR = './data'\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "for dir_ in os.listdir(DATA_DIR):\n",
    "    for img_path in os.listdir(os.path.join(DATA_DIR, dir_)):\n",
    "        data_aux = []\n",
    "\n",
    "        x_ = []\n",
    "        y_ = []\n",
    "\n",
    "        img = cv2.imread(os.path.join(DATA_DIR, dir_, img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = hands.process(img_rgb)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                    x_.append(x)\n",
    "                    y_.append(y)\n",
    "\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "                    data_aux.append(x - min(x_))\n",
    "                    data_aux.append(y - min(y_))\n",
    "\n",
    "            data.append(data_aux)\n",
    "            labels.append(dir_)\n",
    "\n",
    "f = open('data.pickle', 'wb')\n",
    "pickle.dump({'data': data, 'labels': labels}, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9d959b-83b8-425e-9ff9-350a936162c0",
   "metadata": {},
   "source": [
    "***Train The Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca83f883-0031-498e-acf5-098db8821816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of data entries: 84\n",
      "100.0% of samples were classified correctly!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data_dict = pickle.load(open('./data.pickle', 'rb'))\n",
    "\n",
    "data = data_dict['data']\n",
    "labels = data_dict['labels']\n",
    "\n",
    "max_length = max([len(entry) for entry in data])\n",
    "print(f\"Max length of data entries: {max_length}\")\n",
    "\n",
    "fixed_data = []\n",
    "for entry in data:\n",
    "    if len(entry) < max_length:\n",
    "        fixed_data.append(entry + [0] * (max_length - len(entry)))\n",
    "    elif len(entry) > max_length:\n",
    "        fixed_data.append(entry[:max_length])\n",
    "    else:\n",
    "        fixed_data.append(entry)\n",
    "\n",
    "data = np.asarray(fixed_data)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "score = accuracy_score(y_predict, y_test)\n",
    "print('{}% of samples were classified correctly!'.format(score * 100))\n",
    "\n",
    "with open('model.p', 'wb') as f:\n",
    "    pickle.dump({'model': model}, f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75597105-f52c-4b75-bdeb-ed5e2732993b",
   "metadata": {},
   "source": [
    "***Test The Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe2a983-947b-4081-b675-def518606273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swaga\\anaconda3\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n",
      "Insufficient features for prediction.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C', 3: 'D'}\n",
    "\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture video frame.\")\n",
    "        break\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,  \n",
    "                hand_landmarks,  \n",
    "                mp_hands.HAND_CONNECTIONS,  \n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "            # Collect the x and y coordinates of each landmark\n",
    "            x_ = [landmark.x for landmark in hand_landmarks.landmark]\n",
    "            y_ = [landmark.y for landmark in hand_landmarks.landmark]\n",
    "\n",
    "            # Compute the features (normalized x and y differences)\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                data_aux.append(landmark.x - min(x_))\n",
    "                data_aux.append(landmark.y - min(y_))\n",
    "\n",
    "        # Ensure we have exactly 84 features (42 x and 42 y coordinates)\n",
    "        if len(data_aux) == 84:\n",
    "            print(f\"Features: {data_aux}\")  # Print the extracted features\n",
    "\n",
    "            # Predict the hand gesture\n",
    "            prediction = model.predict([np.asarray(data_aux)])\n",
    "            predicted_character = labels_dict[int(prediction[0])]\n",
    "            print(f\"Predicted Character: {predicted_character}\")  # Print the predicted character\n",
    "\n",
    "            # Calculate bounding box to draw around the hand\n",
    "            x1 = int(min(x_) * W) - 10\n",
    "            y1 = int(min(y_) * H) - 10\n",
    "            x2 = int(max(x_) * W) - 10\n",
    "            y2 = int(max(y_) * H) - 10\n",
    "\n",
    "            # Draw a rectangle around the hand\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "\n",
    "            # Add the predicted character above the bounding box\n",
    "            cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "        else:\n",
    "            print(\"Insufficient features for prediction.\")\n",
    "\n",
    "    # Display the frame with the hand landmarks and prediction\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and destroy all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362d71cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
